# -*- coding: utf-8 -*-
"""regression assignmnt

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xTNu_uoYr7SlYotycyJZnDgGJK-Gd089
"""

# Load the California Housing dataset using the fetch_california_housing function from sklearn.
# Convert the dataset into a pandas DataFrame for easier handling.

from sklearn.datasets import fetch_california_housing
import pandas as pd
import numpy as np
# Load dataset
data= fetch_california_housing()

# Convert to DataFrame
x = pd.DataFrame(data.data, columns=data.feature_names)

# Add the target variable (Median House Value)
y = pd.Series(data.target,name="Target")
y

#Handle missing values
null_values=x.isnull().sum()
null_values

"""There is no null values

"""

#checking skewness for detecting outliers
skewness=x.skew()
skewness



"""There is outliers in MedInc,AveRooms,AveBedrms ,Population ,AveOccup
So transform them using Iqr method and capping


"""



#dfine a function to transform outlier using IQR
def cap_outliers_iqr(series):
    """Caps outliers in a pandas Series using IQR."""
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_whisker = Q1 - 1.5 * IQR
    upper_whisker = Q3 + 1.5 * IQR
    return np.clip(series, lower_whisker, upper_whisker)

# Columns to transform
columns_to_transform = ['MedInc', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']

# Apply the capping function to the specified columns
x[columns_to_transform] = x[columns_to_transform].apply(cap_outliers_iqr)

skew=x.skew()
skew

"""Transformed all outliers."""

#feature scaling
from sklearn.preprocessing import StandardScaler
# Create scaler object
scaler = StandardScaler()
# Fit and transform features (x)
x_scaled = scaler.fit_transform(x)
x_scaled



"""**Explanation of Preprocessing Steps**
1) Converted to DataFrame: Easier data handling.

2) Checked for Missing Values: Ensures data quality.

3) Outlier detection and caping:limits their extreme values to improve model accuracy and stability by reducing the influence of anomalous data.

4) Feature Scaling: Standardization ensures that all features contribute equally to model training.

**Regression Algorithm Implementation**
"""

#split the data training and testing
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(x_scaled,y)

"""1)Linear Regression"""

#import modules
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(x_train, y_train)
#making predictions
linear_pred = model.predict(x_test)
linear_pred

"""Linear Regression is a statistical method used for predicting a continuous
target variable based on one or more predictor variables. It assumes a linear relationship between the features and the target, meaning that a change in a feature will result in a proportional change in the target variable.

Linear Regression might be suitable for the California Housing dataset for the following reasons:

**Interpretability**: Linear Regression models are easy to interpret. The coefficients provide insights into the relationship between each feature and the target variable.

**Simplicity:** Linear Regression is a relatively simple algorithm to implement and understand, making it a good starting point for regression tasks.

**Potential Linear Relationships**: While the relationship between housing features and prices might not be perfectly linear, there could be some underlying linear trends that the model can capture.

2)Random Forest Regression:
"""

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(x_train, y_train)
#making predictions
rf_pred = model.predict(x_test)
rf_pred

"""**Explanation**: An ensemble of decision trees, where each tree is trained on a random subset of the data and features. The final prediction is an average of the predictions from all trees.

Suitability for California Housing : Can handle non-linear relationships and interactions between features, making it suitable for the potentially complex housing market data.

3) Decision Tree Regression :
"""

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
model.fit(x_train, y_train)
#making predictions
dt_pred = model.predict(x_test)
dt_pred

"""Explanation: Builds a tree-like structure to make predictions based on feature thresholds. It recursively splits the data into subsets based on feature values to minimize prediction errors.

Suitability for California Housing: Can handle non-linear relationships and interactions between features, making it suitable for the potentially complex housing market data.

4)Gradient Boosting Regressor
"""

from sklearn.ensemble import GradientBoostingRegressor
model = GradientBoostingRegressor()
model.fit(x_train, y_train)
#making predictions
gb_pred = model.predict(x_test)
gb_pred

"""**Explanation**: Combines weak learners (typically decision trees) sequentially, where each learner focuses on correcting the errors of the previous learners. This iterative process improves prediction accuracy.

Suitability for California Housing: Known for high prediction accuracy and can handle various data types and relationships, making it a potentially strong candidate for this dataset

5)Support Vector Regressor (SVR)
"""

from sklearn.svm import SVR
model = SVR()
model.fit(x_train, y_train)
#making predictions
svr_pred = model.predict(x_test)
svr_pred

"""Explanation: Uses support vectors to define a hyperplane that best separates the data points and predicts the target variable. It can handle non-linear relationships using kernel functions.


Suitability for California Housing: Effective for non-linear relationships and robust to outliers, which might be present in housing data.

MODEL EVALUATION AND COMPARISON

Evaluate the performance of each algorithm using the following metrics:
Mean Squared Error (MSE)

Mean Absolute Error (MAE)

R-squared Score (RÂ²)
"""

#Import necessary module
from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score



"""1)Linear regression model

"""

#mean squared error metric
mse_linear = mean_squared_error(y_test, linear_pred)
#mean absolute error metric
mae_linear = mean_absolute_error(y_test, linear_pred)  # Calculate MAE
#r2 score
r2_linear = r2_score(y_test, linear_pred)

#PRINT THE RESULT
print("Linear Regression Evaluation : ")
print("Mean_squared Error : ",mse_linear)
print("Mean_Absolute Error",mae_linear)
print("R2 Score ",r2_linear)

"""2)Random Forest Regression:"""

#mean squared error metric
mse_rf = mean_squared_error(y_test, rf_pred)
#mean absolute error metric
mae_rf= mean_absolute_error(y_test, rf_pred)
#r2 score
r2_rf = r2_score(y_test, rf_pred)

#PRINT THE RESULT
print("Random Forest Regression Evaluation : ")
print("Mean_squared Error : ",mse_rf)
print("Mean_Absolute Error" ,mae_rf)
print("R2 Score ",r2_rf)

"""3) Decision Tree Regression :"""

#mean squared error metric
mse_dt = mean_squared_error(y_test, dt_pred)
#mean absolute error metric
mae_dt= mean_absolute_error(y_test, dt_pred)
#r2 score
r2_dt = r2_score(y_test, dt_pred)

#PRINT THE RESULT
print("Decision Tree Regression Evaluation : ")
print("Mean_squared Error : ",mse_dt)
print("Mean_Absolute Error" ,mae_dt)
print("R2 Score ",r2_dt)

"""4)Gradient Boosting Regressor"""

#mean squared error metric
mse_gb = mean_squared_error(y_test, gb_pred)
#mean absolute error metric
mae_gb= mean_absolute_error(y_test, gb_pred)
#r2 score
r2_gb = r2_score(y_test, gb_pred)

#PRINT THE RESULT
print("Decision Tree Regression Evaluation : ")
print("Mean_squared Error : ",mse_gb)
print("Mean_Absolute Error" ,mae_gb)
print("R2 Score ",r2_gb)

"""5)Support Vector Regressor (SVR)"""

#mean squared error metric
mse_svr = mean_squared_error(y_test, svr_pred)
#mean absolute error metric
mae_svr= mean_absolute_error(y_test, svr_pred)
#r2 score
r2_svr= r2_score(y_test, svr_pred)

#PRINT THE RESULT
print("Support Vector Regression Evaluation : ")
print("Mean_squared Error : ",mse_svr)
print("Mean_Absolute Error" ,mae_svr)
print("R2 Score ",r2_svr)

"""**Comparing all 5 models :**"""

model_comparison = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'Decision Tree', 'Gradient Boosting', 'SVR'],
    'MSE': [mse_linear, mse_rf, mse_dt, mse_gb, mse_svr],
    'MAE': [mae_linear, mae_rf, mae_dt, mae_gb, mae_svr],
    'R-squared': [r2_linear, r2_rf, r2_dt, r2_gb, r2_svr]
})

model_comparison

"""Best Performing Algorithm

Based on R-squared: The model with the highest R-squared value is generally the best. R-squared represents the proportion of variance in the target variable that is explained by the model. A higher R-squared indicates a better fit.

Based on MSE and MAE: The model with the lowest MSE and MAE values is preferred. These metrics measure the average error between the predicted and actual values. Lower values indicate better accuracy.

In this project,**Random Forest Regression** has high **highest R-squared value** and **lowest MSE and MAE values ** .Therefore,Random Forest is the best performing algorithm.

At the same time,**Linear Regression** has **lowest R-squared value** and **highest MSE** and **MAE** values.Therefore,Linear reression is the worst performing algorithm.
"""